{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import sys\n",
    "sys.path.append('../scripts/')\n",
    "\n",
    "# Our helper, with the functions: \n",
    "# plot_vector, plot_linear_transformation, plot_linear_transformations\n",
    "from plot_helper import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eckart–Young Theorem & Low Rank Approximation\n",
    "\n",
    "Consider the $4 \\times 4$ matrix\n",
    "\n",
    "$ A = \\begin{bmatrix} 1 & 2 & 3 & 6 \\\\\n",
    "                      2 & 5 & 7 & 10 \\\\\n",
    "                      3 & 9 & 12 & 14 \\\\\n",
    "                      4 & 7 & 9 & 15 \\\\\n",
    "                      \\end{bmatrix} $\n",
    "                      \n",
    "The rank of $A$ is $4$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2, 3, 6],\n",
    "              [2, 5, 7, 10],\n",
    "              [3, 9, 12, 14],\n",
    "              [4, 7, 9, 15]])\n",
    "\n",
    "print(numpy.linalg.matrix_rank(A))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A$ has exactly four non-zero singular values, another justification for the fact that rank of $A$ is four. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31.94504943  2.81273628  0.77396598  0.05751823]\n"
     ]
    }
   ],
   "source": [
    "U, S, VT = numpy.linalg.svd(A)\n",
    "print(S)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last singular value $\\sigma_4 = 0.05751823$ is significantly smaller than the others and is close to zero. $A$ has full rank but is very close to a rank-$3$ matrix. By dropping the smallest singular value and its singular vectors in the decomposition, we arrive at a rank-$3$ truncation $\\tilde{A}_3$ of matrix $A$. \n",
    "\n",
    "\n",
    "$$ \\tilde{A_3} = \\sigma_1 u_1 v_1^T +  \\sigma_2 u_2 v_2^T +  \\sigma_3 u_3 v_3^T $$\n",
    "\n",
    "As we shall learn in the following section, the truncated matrix $\\tilde{A_3}$ is the best approximation to $A$ among all rank $3$ matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_trunc (matrix, level) : \n",
    "    U, S, VT = numpy.linalg.svd(matrix)\n",
    "    matrix = U[:,:level] @ numpy.diag(S[:level]) @ VT[:level,:]\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.01333778  1.9779498   3.0176867   5.99611842]\n",
      " [ 1.98096577  5.03146764  6.97475947 10.00553936]\n",
      " [ 3.00726293  8.98799282 12.00963108 13.99788633]\n",
      " [ 4.00058714  6.99902932  9.00077859 14.99982913]]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "A_3 = svd_trunc(A, 3) \n",
    "\n",
    "print (A_3)\n",
    "\n",
    "print(numpy.linalg.matrix_rank(A_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.4666304   2.18725118  2.90681548  5.85953327]\n",
      " [ 2.28645823  5.17252441  6.90003882  9.91348904]\n",
      " [ 3.00573676  8.98728813 12.01000437 13.9983462 ]\n",
      " [ 3.62585599  6.82600258  9.09243439 15.11274231]]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "A_2 = svd_trunc(A, 2)\n",
    "\n",
    "print (A_2)\n",
    "\n",
    "print(numpy.linalg.matrix_rank(A_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.17291925  2.72752312  3.63917489  5.11820073]\n",
      " [ 2.25177822  5.23631714  6.98651232  9.82595603]\n",
      " [ 3.48539152  8.10497908 10.8140005  15.20900393]\n",
      " [ 3.24034329  7.53514042 10.05369803 14.13970093]]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "A_1 = svd_trunc(A, 1)\n",
    "\n",
    "print(A_1)\n",
    "\n",
    "print(numpy.linalg.matrix_rank(A_1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though $\\tilde{A}_3$ is rank-deficient, each entry is very close to the full-rank matrix $A$. The example might look trivial, but it demonstrates the important idea that we can sometimes approximate a higher-rank matrix with a reduced-rank matrix to save storage and transmission costs. This strategy is called *low-rank approximation*, and you will find that it's ubiquitous in scientific computing.\n",
    "\n",
    "**Warning**: Rounding errors may lead to small but non-zero singular values in a rank deficient matrix. Singular values that are smaller than a given tolerance are assumed to be numerically equivalent to zero, defining what is sometimes called the **effective rank**.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD Reminder  \n",
    "\n",
    "Suppose $A$ is an $m \\times n$ matrix with real entries. Perform the the singular value decomposition to obtain $ A = U\\Sigma V^T$. Recall that $U$ and $V$ are orthogonal matrices, and $\\Sigma$ is an $m\\times n$ diagonal matrix with entries $(\\sigma_{1}, \\sigma_{2}, \\cdots ,\\sigma_{n})$ such that $\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq \\sigma_n\\geq 0$.\n",
    "\n",
    "\n",
    "\n",
    "Recall that the singular values of $A$ (i.e. the non-zero diagonal entries of $\\Sigma$) are the square-root of eigenvalues of $A^T \\, A$. The matrix $A^T \\, A$ is an $n \\times n$ matrix and, therefore, it has $n$ eigenvalues. We have as many singular values and singular vectors as the number of columns of $A$. This is because, we have $n$ = \"the number of columns of $A$\" is the dimension of the the domain of the linear transformation $A : \\mathbb{R}^n \\to \\mathbb{R}^m$ (which is $ \\mathbb{R}^n$)  \n",
    "\n",
    "$V$ is an $n \\times n$ orthogonal matrix whose columns are eigenvectors of $A^T \\, A$. The columns of $V$ are called the *right singular vectors* of $A$.\n",
    "\n",
    "$U$ is an $m \\times m$ orthogonal matrix whose columns are eigenvectors of $A \\, A^T$. The columns of $U$ are called the *left singular vectors* of $A$.\n",
    "\n",
    "\n",
    "Suppose $r = \\mathrm{rank}(A)$. We have that $r \\le \\mathrm{min}(m,n)$. We analyze the three cases of the shape of SVDs based on the comparison of $m$ and $n$. In both cases, \n",
    "\n",
    "$$ A v_i = \\sigma_i u_i \\, \\text{for all} \\,  i \\le r \\, \\, \\text{and} \\, \\, A v_i =  0 \\, \\text{forall} \\, i > r $$ \n",
    "\n",
    "In all cases, we will have $r$ positive singular values in descending order: \n",
    "\n",
    "$$\\sigma_1 \\ge \\sigma_2 \\ge \\ldots \\ge \\sigma_r > 0  $$\n",
    "\n",
    "\n",
    "And in all cases, \n",
    "\n",
    "$$ A = \\sigma_1 u_1 v_1^T + \\ldots + \\sigma_r u_r v_r^T $$ \n",
    "\n",
    "The remaining singular values are zero. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three Cases of SVD Shapes\n",
    "\n",
    "\n",
    "1. **More columns than rows**: If $m < n$ then the SVD of $A$ looks like \n",
    "\n",
    "$$\n",
    "  A =\n",
    "    \\begin{bmatrix}\n",
    "    \\mathbf{u_1} & \\mathbf{u_2} & \\dots & \\mathbf{u_m}\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    \\sigma_1 & & & & 0 & \\ldots & 0  \\\\\n",
    "    & \\sigma_2 & & & 0 & \\ldots & 0  \\\\\n",
    "    & & \\ddots  & & 0 & \\ldots & 0 \\\\\n",
    "    & & & \\sigma_m & 0 & \\ldots & 0\\\\\n",
    "    \\end{bmatrix}   \n",
    "    \\begin{bmatrix}\n",
    "    \\mathbf{v_1}^T \\\\\n",
    "    \\mathbf{v_2}^T \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{v_n}^T\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$m -r$ singular values are zero. In the diagonal matrix $\\Sigma$ we usually order the singular values in terms of their magnitude from the top to the bottom. Therefore, all the zero $\\sigma$'s come after all positive $\\sigma$'s. The above decomposition can be rewritten as\n",
    "\n",
    "$$\n",
    "  A =\n",
    "    \\begin{bmatrix}\n",
    "    \\mathbf{u_1} & \\mathbf{u_2} & \\dots & \\mathbf{u_m}\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    \\sigma_1 & & & & & 0 & \\ldots & 0  \\\\\n",
    "      & \\ddots  & &  & & 0 & \\ldots & 0 \\\\\n",
    "    & & \\sigma_r  & & &  \\vdots & \\ldots & \\vdots \\\\\n",
    "    & &  & \\ddots  & &  0 & \\ldots & 0 \\\\\n",
    "    & & & & 0 & 0 & \\ldots & 0\\\\\n",
    "    \\end{bmatrix}   \n",
    "    \\begin{bmatrix}\n",
    "    \\mathbf{v_1}^T \\\\\n",
    "    \\mathbf{v_2}^T \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{v_n}^T\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "On the diagonal entries whatever comes after $\\sigma_r$ is going to be zero, i.e. $\\sigma_{r+1} = \\ldots \\sigma_m = 0$.\n",
    "\n",
    "\n",
    "2. **More rows than columns**: If $m > n$ then the SVD of $A$ looks like \n",
    "\n",
    "\n",
    "$$\n",
    "  A =\n",
    "    \\begin{bmatrix}\n",
    "    \\mathbf{u_1} & \\mathbf{u_2} & \\dots & \\mathbf{u_m}\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    \\sigma_1 & & & &   \\\\\n",
    "      & \\ddots  & &  &  \\\\\n",
    "    & & \\sigma_r  & &  \\\\\n",
    "    & &  & \\ddots  &  \\\\\n",
    "    & & & & 0 \\\\\n",
    "    0 & & \\ldots & & 0 \\\\\n",
    "    \\vdots & & \\ddots & & \\vdots \\\\ \n",
    "    0 & & \\ldots & & 0 \\\\\n",
    "    \\end{bmatrix}   \n",
    "    \\begin{bmatrix}\n",
    "    \\mathbf{v_1}^T \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{v_n}^T\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "**Same number of rows as columns**: If $m = n$ then the SVD of $A$ looks like \n",
    "\n",
    "$$\n",
    "  A =\n",
    "    \\begin{bmatrix}\n",
    "    \\mathbf{u_1} &  \\dots & \\mathbf{u_m}\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    \\sigma_1 & & & &   \\\\\n",
    "      & \\ddots  & &  &  \\\\\n",
    "    & & \\sigma_r  & &  \\\\\n",
    "    & &  & \\ddots  &  \\\\\n",
    "    & & & & 0 \\\\\n",
    "    \\end{bmatrix}   \n",
    "    \\begin{bmatrix}\n",
    "    \\mathbf{v_1}^T \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{v_m}^T\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "where the diagonal matrix $\\Sigma$ is a square matrix.\n",
    "\n",
    "\n",
    "In all of the above cases: \n",
    "\n",
    "- $V$ is an  orthogonal $n \\times n$ matrix whose columns $v_i$ are perpendicular to each other. Note that $n - r$ of the vectors $v_i$ are in the null-space of $A$, indeed $A v_{r +1} = \\ldots = A v_n = 0$. \n",
    "\n",
    "- $m - r$ vectors $u_{r + 1}, \\ldots, u_m$ are in the null-space of $A^T$, since \n",
    "$$ A^T = V \\Sigma^T U^T$$ \n",
    "and therefore, for all $i > r$ \n",
    "$$A^T u_i = \\sigma_1 v_1 u_1^T u_i + \\ldots + \\sigma_r v_r u_r^T u_i  = 0$$ \n",
    "because all the inner products $\\langle u_1, u_i  \\rangle  = u_1^T u_i, \\ldots, \\langle u_r, u_i  \\rangle = u_r^T u_i$ are zero. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The SVD Truncation of Matrices \n",
    "\n",
    "Suppose $k \\le r = \\mathrm{rank}(A)$. The SVD **$k$-truncation** of $A$ is given by the following $k$-truncated sum of the rank one matrices. \n",
    "\n",
    "$$\n",
    "\\tilde{A_k} := \\sum_{i=1}^k \\sigma_i u_i v_i^T\n",
    "$$\n",
    "\n",
    "Equivalently, $\\tilde{A_k}$ is equal to the product $U_k \\Sigma_k V_k$ of shapes ${m \\times k}$, ${k \\times k}$ and ${k \\times m}$. \n",
    "\n",
    "$$\n",
    "  \\tilde{A_k} =\n",
    "    \\begin{bmatrix}\n",
    "    \\mathbf{u_1} &  \\dots & \\mathbf{u_k}\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    \\sigma_1 & &    \\\\\n",
    "      & \\ddots  &     \\\\\n",
    "    & & \\sigma_k     \\\\\n",
    "    \\end{bmatrix}   \n",
    "    \\begin{bmatrix}\n",
    "    \\mathbf{v_1}^T \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{v_k}^T\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note that this is in the reduced form and the full SVD of $\\tilde{A_k}$ should have square matrices for $U$ and $V$. The full SVD of $\\tilde{A_k} = \\tilde{U_k} \\tilde{\\Sigma_k} \\tilde{V_k}$ is given as \n",
    "\n",
    "$$\n",
    "  \\tilde{A_k} =\n",
    "    \\begin{bmatrix}\n",
    "    \\mathbf{u_1} &  \\dots & \\mathbf{u_k} & \\dots & \\mathbf{u_m} \n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    \\sigma_1 & & &  0 & \\ldots & 0  \\\\\n",
    "      & \\ddots  & &   0 & \\ldots & 0 \\\\\n",
    "    & & \\sigma_k  &   \\vdots & \\ldots & \\vdots \\\\\n",
    "    0  & \\ldots & 0 &  0 & \\ldots & 0\\\\\n",
    "    \\vdots & \\ldots & 0 & 0  &  \\ldots & 0 \\\\\n",
    "    0 & \\ldots & 0 & 0 &  \\ldots & 0\\\\\n",
    "    \\end{bmatrix}    \n",
    "    \\begin{bmatrix}\n",
    "    \\mathbf{v_1}^T \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{v_k}^T \\\\ \n",
    "    \\vdots \\\\ \n",
    "    \\mathbf{v_n}^T\n",
    "    \\end{bmatrix} = \n",
    "    U \\tilde{\\Sigma_k} V^T\n",
    "$$\n",
    "\n",
    "In practice though, we would like to not have to store and transmit all the useless $0$ in the matrix $\\Sigma$ above. That is what the reduced SVD as opposed to full SVDs are used in practice.  \n",
    "\n",
    "\n",
    "The $k$-truncated matrix $\\tilde{A_k}$ contains the $k$ most important pieces of $A$, namely $\\sigma_1 u_1 v_1^T, \\ldots, \\sigma_k u_k v_k^T$. The Eckart–Young theorem tells us that the $k$-truncated matrix $\\tilde{A_k}$ is the best rank-$k$ approximation to $A$ in the sense that $ \\| A - \\tilde{A_k} \\|$ is minimized among all rank $\\le k$ matrix. In other words, for all matrices $B$ of rank $k$ or less,  \n",
    "\n",
    "$$  \\| A - \\tilde{A_k} \\| \\le \\| A - B \\| $$ \n",
    "\n",
    "The norm for which this equality works is one of the followings: \n",
    "- $L^2$ norm of a matrix $M =$  $\\| M \\|_2 = \\sigma_1$ \n",
    "- The Frobenius norm of a matrix $\\| M \\|_\\text{F} = \\big(\\sum_{i=1}^{r} \\sigma_i^2\\big)^{1/2} $  \n",
    "- The nuclear norm of a matrix $\\| M \\|_\\text{F} = \\sum_{i=1}^{r} \\sigma_i $  (check that this is indeed a norm. Is this norm induced by an inner product? What is this inner product?)\n",
    "\n",
    "Note that the matrix $\\tilde{A_k}$ has rank $k$ since its SVD returns exactly non-zero singular values. Therefore, \n",
    "\n",
    "\n",
    "$$ \\|A - \\tilde{A_k} \\| = \\min_{\\operatorname{rank}(B) \\leq k} \\| A - B \\|  $$\n",
    "\n",
    "We can actually calculate this minimum: \n",
    "\n",
    "- For the $L^2$-norm we have: \n",
    "$ \n",
    "\\|A - \\tilde{A_k} \\|_2  =  \\| U \\Sigma V^T - U \\tilde{\\Sigma_k} V^T \\|_2 = \\|  U (\\Sigma - \\tilde{\\Sigma_k} ) V^T \\| = \\|_2 = \\| \\Sigma - \\tilde{\\Sigma_k} \\|_2 = \\sigma_{k + 1}  \n",
    "$\n",
    "\n",
    "- For the Frobenius norm we have \n",
    "$\n",
    "\\|A - \\tilde{A_k} \\|_\\text{F}  =  \\big(\\sigma_{k+1}^2 + \\ldots + \\sigma_r^2 \\big)^{1/2} \n",
    "$\n",
    "\n",
    "- For the nuclear norm we have \n",
    "$\n",
    "\\|A - \\tilde{A_k} \\|_\\text{nuclear}  =  \\sigma_{k+1} + \\ldots + \\sigma_r\n",
    "$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof of Eckart-Young for the $L^2$ norm\n",
    "\n",
    "\n",
    "We want to show that $\\|A - \\tilde{A_k} \\| = \\min_{\\operatorname{rank}(B) \\leq k} \\| A - B \\|$. Take a matrix $B$ of rank at most $k$. If $k = r = \\mathrm{rank}(A)$ then \n",
    "\n",
    "$$ \\min_{\\operatorname{rank}(B) \\leq k} \\| A - B \\| =  \\min_{\\operatorname{rank}(B) \\leq \\operatorname{rank}(A)} \\| A - B \\| = 0 = \\|A - \\tilde{A_r} \\|$$\n",
    "since we can $B$ to be $A$ itself. Therefore, we only need to prove the theorem in case $k < r \\le n$. Therefore $n - k \\ge 1$. By the rank-nullity theorem, the null-space of $B$ has at least $n-k \\ge 1$ dimensions. Consider the first $k + 1$ right singular vectors $v_1, \\ldots , v_{k+1}$ of $A$ and let $W_{k+1}$ be the space spanned by these vectors. $\\mathrm{dim}(W) = k+1$. By an easy dimension analysis, there must be a non-zero vector $w$ in the intersection $W \\cap \\mathrm{null}(B)$. Therefore, there must be a nontrivial linear combination of the first $k+1$ columns of $V$, i.e.,\n",
    "\n",
    "$$\n",
    "w = c_1 v_1 + \\cdots+ c_{k+1} v_{k+1},\n",
    "$$\n",
    "\n",
    "such that $B w = 0$. Without loss of generality, we can scale $w$ so that $\\|w\\|_2 = 1$ or (equivalently) $c_1^2+\\cdots +c_{k+1}^2 = 1$. Therefore,\n",
    "\n",
    "$$\n",
    "\\|A-B\\|_2^2 = \\max_{w, \\, \\| w \\| = 1} \\|(A-B)w\\|_2^2  \\geq \\|(A-B)w\\|_2^2 = \\|Aw\\|_2^2 = c_1^2\\sigma_1^2+\\cdots+c_{k+1}^2\\sigma_{k+1}^2\\geq \\sigma_{k+1}^2 = \\| A - \\tilde{A_k} \\|_2^2.\n",
    "$$\n",
    "\n",
    "The result follows by taking the square root of both sides of the above inequality."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to the example at the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05751823344612"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(A - A_3, ord=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which verifies Eckart-Young theorem, since `0.05751823344612` is exactly the fourth (and the last) singular value of $A$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7739659825291152"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(A - A_2, ord=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and this is the third largest singular value of $A$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Frobenius norm $\\|A - \\tilde{A_k} \\|_\\text{F}$  can also be minimized too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7761003087816812"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(A - A_2, ord='fro')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof of Eckart-Young for the Frobenius norm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcfds_LinAlg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
